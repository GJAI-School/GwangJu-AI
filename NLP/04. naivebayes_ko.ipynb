{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "스토리가 점점 산으로가네..\n",
      "스토리가 점점 산으로가네..가 부정적일 확률 : 0.981757353504198, 긍정적일 확률 : 0.018242646495802057\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import csv\n",
    "from math import log, exp\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "def start():\n",
    "    train_datas = open_csv()\n",
    "    test_data = input()\n",
    "    prob = naive_bayes(train_datas, test_data, 0.5, 0.5)\n",
    "\n",
    "    print(f'{test_data}가 부정적일 확률 : {prob[0]}, 긍정적일 확률 : {prob[1]}')\n",
    "    \n",
    "def open_csv():\n",
    "    f = open('./data/ratings.txt', 'r', encoding='utf-8')\n",
    "    csvreader = csv.reader(f)\n",
    "\n",
    "    pos_doc = []\n",
    "    neg_doc = []\n",
    "\n",
    "    # 첫줄은 헤더라서 스킵\n",
    "    next(csvreader)\n",
    "    for line in csvreader:\n",
    "        if line[-1][-1] == '1':\n",
    "            pos_doc.append(line[0][:-2])\n",
    "        else:\n",
    "            neg_doc.append(line[0][:-2])\n",
    "\n",
    "    train_datas = [[], []]\n",
    "    train_datas[0] = neg_doc\n",
    "    train_datas[1] = pos_doc\n",
    "\n",
    "    # 리스트 형태는 토큰화하기가 어렵기 때문에, 전부 조인을 통해서 하나의 문자열로 만들어준다\n",
    "    return [' '.join(train_datas[0]), ' '.join(train_datas[1])]\n",
    "\n",
    "def calculate_doc_prob(train_data, test_data, nowords_weight):\n",
    "    sw_train_data = re.compile('[^\\w]').sub(' ', train_data.lower())\n",
    "    sw_train_token = sw_train_data.split()\n",
    "    train_vector = dict(Counter(sw_train_token))\n",
    "\n",
    "    sw_test_data = re.compile('[^\\w]').sub(' ', test_data.lower())\n",
    "    sw_test_token = sw_test_data.split()\n",
    "    test_vector = dict(Counter(sw_test_token))\n",
    "\n",
    "    log_prob = 0\n",
    "\n",
    "    total_wc = len(sw_train_token)\n",
    "\n",
    "    for word in test_vector:\n",
    "        if word in train_vector:\n",
    "            log_prob += log(train_vector[word]/total_wc)\n",
    "        else:\n",
    "            log_prob += log(nowords_weight/total_wc)\n",
    "\n",
    "    return log_prob\n",
    "\n",
    "def naive_bayes(train_datas, test_data, pos_prob, neg_prob):\n",
    "    test_pos_prob = calculate_doc_prob(train_datas[1], test_data, 0.1) + log(pos_prob) \n",
    "    test_neg_prob = calculate_doc_prob(train_datas[0], test_data, 0.1) + log(neg_prob) \n",
    "\n",
    "    maxprob = max(test_neg_prob, test_pos_prob) \n",
    "    test_pos_prob -= maxprob \n",
    "    test_neg_prob -= maxprob\n",
    "    test_pos_prob = exp(test_pos_prob)\n",
    "    test_neg_prob = exp(test_neg_prob) \n",
    "    normalized_prob = [test_neg_prob/(test_pos_prob+test_neg_prob), test_pos_prob/(test_pos_prob+test_neg_prob)]\n",
    "\n",
    "    return normalized_prob\n",
    "\n",
    "start()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
